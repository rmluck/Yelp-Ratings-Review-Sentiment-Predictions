{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f060f643",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "\n",
    "We imported all of the libraries needed for the project below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "249ce920",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/cs179/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import pyro\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyro.distributions as dist\n",
    "from pyro.nn import PyroModule, PyroSample\n",
    "from pyro.infer import SVI, Trace_ELBO, Predictive\n",
    "from pyro.optim import Adam\n",
    "from pyro.infer.autoguide import AutoNormal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43812060",
   "metadata": {},
   "source": [
    "## Load and Prepare Data\n",
    "\n",
    "We just read in the CSV file that was created earlier (``ca_restaurants_bayesian_dataset.csv``):\n",
    "business_id,name,avg_rating,avg_sentiment_score,log_review_count\n",
    "\n",
    "After loading the data into a Pandas DataFrame, we extract the columns named `avg_sentiment_score`, `log_review_count`, and `avg_rating` (these were really the only parts of the csv file we needed for our model). We then convert these columns into tensors so that they can be fed directly into our Pyro model. The tensor `X_sentiment` holds each restaurant’s sentiment score, `X_log_reviews` holds the log review counts, and `y` holds the target ratings we want to predict.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "406565c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv(\"ca_restaurants_bayesian_dataset.csv\")\n",
    "\n",
    "# Extract the features and target\n",
    "X_sentiment = torch.tensor(df[\"avg_sentiment_score\"].values, dtype=torch.float)\n",
    "X_log_reviews = torch.tensor(df[\"log_review_count\"].values, dtype=torch.float)\n",
    "y = torch.tensor(df[\"avg_rating\"].values, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0019b67c",
   "metadata": {},
   "source": [
    "## Define the Bayesian Model\n",
    "\n",
    "The Bayesian linear regression model assumes that each restaurant’s rating just comes from a normal distribution whose mean depends on two inputs (the average review sentiment and the log review count).\n",
    "\n",
    "We first assign prior distributions to each parameter:\n",
    "\n",
    "- `alpha` is the intercept, drawn from a Normal(0, 1) prior.\n",
    "- `beta_sentiment` is the slope for sentiment score, drawn from Normal(0, 1).\n",
    "- `beta_reviews` is the slope for log review count, drawn from Normal(0, 1).\n",
    "- `sigma` is the standard deviation of the noise, drawn from a HalfNormal(1) prior.\n",
    "\n",
    "We then do `mean` = `alpha` + `beta_sentiment * X_sentiment` + `beta_reviews * X_log_reviews`. Each observed rating `y` is sampled from a Normal distribution centered at `mean` with standard deviation `sigma`. Pyro will just use this to compare model predictions against the actual ratings during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81956082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Bayesian model\n",
    "def bayesian_model(X_sentiment, X_log_reviews, y=None):\n",
    "    # Define priors for the model parameters\n",
    "    alpha = pyro.sample(\"alpha\", dist.Normal(0.0, 1.0))\n",
    "    beta_sentiment = pyro.sample(\"beta_sentiment\", dist.Normal(0.0, 1.0))\n",
    "    beta_log_reviews = pyro.sample(\"beta_reviews\", dist.Normal(0.0, 1.0))\n",
    "    sigma = pyro.sample(\"sigma\", dist.HalfNormal(1.0))\n",
    "\n",
    "    # Define the linear model\n",
    "    mean = alpha + beta_sentiment * X_sentiment + beta_log_reviews * X_log_reviews\n",
    "    \n",
    "    # Sample from the likelihood\n",
    "    with pyro.plate(\"data\", len(X_sentiment)):\n",
    "        pyro.sample(\"obs\", dist.Normal(mean, sigma), obs=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01c9925",
   "metadata": {},
   "source": [
    "## Create an Automatic Guide\n",
    "\n",
    "This just automatically creates a normal distribution for each model parameter, allowing SVI to learn an approximate posterior without manually defining each variational distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f2a19c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the guide for mean-field variational inference\n",
    "guide = AutoNormal(bayesian_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd14ec3a",
   "metadata": {},
   "source": [
    "Fit the model with stochastic variational inference. Train the model with 5000 steps and print the loss.\n",
    "## Train the Model with Variational Inference\n",
    "\n",
    "Training process yay:\n",
    "\n",
    "We set up an optimizer with a learning rate of 0.01 and tell Pyro to use the evidence lower bound as the loss function for variational inference. We then run a loop for 5000 steps. On each step Pyro updates the guide’s parameters to minimize the bound. Every 500 steps we also go ahead and print out the current loss so that we can track how training progresses toward convergence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f8abe27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 : loss = 5714.16952023888\n",
      "Step 500 : loss = 529.0181514024734\n",
      "Step 1000 : loss = 158.23740577697754\n",
      "Step 1500 : loss = 82.58124113082886\n",
      "Step 2000 : loss = 122.8308812379837\n",
      "Step 2500 : loss = 80.42134487628937\n",
      "Step 3000 : loss = 90.96460288763046\n",
      "Step 3500 : loss = 83.69466906785965\n",
      "Step 4000 : loss = 83.60319823026657\n",
      "Step 4500 : loss = 84.94525158405304\n"
     ]
    }
   ],
   "source": [
    "# Clear the parameter store\n",
    "pyro.clear_param_store()\n",
    "\n",
    "# Define the Stochastic Variational Inference (SVI) object\n",
    "svi = SVI(\n",
    "    model=bayesian_model,\n",
    "    guide=guide,\n",
    "    optim=Adam({\"lr\": 0.01}),\n",
    "    loss=Trace_ELBO()\n",
    ")\n",
    "\n",
    "# Training the model with 5000 steps\n",
    "num_steps = 5000\n",
    "for step in range(num_steps):\n",
    "    # Perform a single step of optimization\n",
    "    loss = svi.step(X_sentiment, X_log_reviews, y)\n",
    "    \n",
    "    # Print the loss every 500 steps\n",
    "    if step % 500 == 0:\n",
    "        print(f\"Step {step} : loss = {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5ff89c",
   "metadata": {},
   "source": [
    "## Inspect Posterior Samples\n",
    "\n",
    "We examine the fitted model by drawing samples from the approx posterior distribution of each parameter.\n",
    "\n",
    "Then once we have the samples, we just compute the mean and a 95 percent credible interval for each parameter by taking the 2.5th and 97.5th percentiles. Printing these values shows our best estimates for how sentiment and review count influence the predicted rating, along with how uncertain those estimates are.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a776f0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha: mean = 1.951, 95% confidence interval = (np.float32(1.9303899), np.float32(1.9696169))\n",
      "beta_sentiment: mean = 3.032, 95% confidence interval = (np.float32(3.0053725), np.float32(3.0571089))\n",
      "beta_reviews: mean = -0.024, 95% confidence interval = (np.float32(-0.027778307), np.float32(-0.019841082))\n",
      "sigma: mean = 0.264, 95% confidence interval = (np.float32(0.25002992), np.float32(0.27815536))\n"
     ]
    }
   ],
   "source": [
    "# Extract the learned parameters\n",
    "predictive = Predictive(\n",
    "    bayesian_model,\n",
    "    guide=guide,\n",
    "    num_samples=1000,\n",
    "    return_sites=[\"alpha\", \"beta_sentiment\", \"beta_reviews\", \"sigma\"]\n",
    ")\n",
    "\n",
    "# Generate samples from the posterior predictive distribution\n",
    "samples = predictive(X_sentiment, X_log_reviews)\n",
    "\n",
    "for parameter in [\"alpha\", \"beta_sentiment\", \"beta_reviews\", \"sigma\"]:\n",
    "    values = samples[parameter].detach().numpy()\n",
    "    mean = values.mean()\n",
    "    confidence_interval = (np.percentile(values, 2.5), np.percentile(values, 97.5))\n",
    "    print(f\"{parameter}: mean = {mean:.3f}, 95% confidence interval = {confidence_interval}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e8f52d",
   "metadata": {},
   "source": [
    "## Split Data into Training and Test Sets\n",
    "\n",
    "We will randomly split it so that 80% of the examples are used for training and 20% for test. This ensures that our model is evaluated on data it has not seen during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4875fe28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 547\n",
      "Test set size: 137\n"
     ]
    }
   ],
   "source": [
    "# Determine the number of examples\n",
    "n = X_sentiment.shape[0]\n",
    "\n",
    "# Create a random permutation of indices from 0 to n-1\n",
    "indices = torch.randperm(n)\n",
    "\n",
    "# Split index at 80% of the data\n",
    "train_size = int(0.8 * n)\n",
    "train_idx = indices[:train_size]\n",
    "test_idx = indices[train_size:]\n",
    "\n",
    "# Create training tensors\n",
    "X_train_sentiment = X_sentiment[train_idx]\n",
    "X_train_log_reviews = X_log_reviews[train_idx]\n",
    "y_train = y[train_idx]\n",
    "\n",
    "# Create test tensors\n",
    "X_test_sentiment = X_sentiment[test_idx]\n",
    "X_test_log_reviews = X_log_reviews[test_idx]\n",
    "y_test = y[test_idx]\n",
    "\n",
    "# Print the sizes to confirm\n",
    "print(f\"Training set size: {train_size}\")\n",
    "print(f\"Test set size: {n - train_size}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs179",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
